🔹 Java 基础 & JVM

**你在简历中提到“熟悉虚拟线程”，能否说一下虚拟线程和传统线程在实现上的区别？在高并发场景下为什么虚拟线程更有优势？**
平台线程直接由操作系统内核调度，切换涉及用户态内核态的转换，开销大；而虚拟线程则是由JVM内部调度，运行在少量的载体线程上，本质上是用户态线程而不需要频繁陷入内核态；平台线程可以管理虚拟线程，一个平台线程可以在不同的时间执行不同的虚拟线程；平台线程启动时会分配固定大小的栈内存，大量创建线程会耗尽内存，而虚拟线程的栈是按需分配的，存在堆里，能够轻量化创建百万级线程。

在高并发场景下传统线程的频繁切换会导致性能急剧下降；虚拟线程的带哦都是协作式的，极大降低上下文切换开销；虚拟线程适合IO密集型场景，如Web服务、RPC调用和数据库访问等。

**你在 RPC 框架中使用了反射来调用方法，能否解释一下反射的性能问题体现在哪里？你会如何优化？**

反射的性能问题体现在：1 调用反射方法时JVM默认会进行安全检查，比如验证是否有权限调用私有方法。2 反射调用绕过编译器优化，无法享受JIT的内联优化 3 每次调用都需要动态查找和参数装箱/拆箱（反射的方法的参数列表被定义为Object[]类型，只能接收对象） 4 反射调用过程中会产生大量的临时对象，这些对象会占用内存，可能会导致频繁gc，从而影响性能。

**Java 内存模型（JMM）中的 volatile 和 synchronized 的区别？你会在什么场景下选择哪一个？**

volatile用于保证可见性和禁止指令重排，但不保证原子性；其底层实现是通过内存屏障实时刷新工作内存到主内存中的；适用于状态标志、单例双重检查锁的instance，适合读多写少的场景
synchronized则可以保证原子性、可见性和有序性；其使用对象头中的markword实现锁的升级，使用对象头中的Monitor监视器


🔹 Spring / 微服务框架

**你提到在国土卫星遥感应用中心平台中使用了 Spring Cloud Alibaba + Nacos + OpenFeign，请你详细描述一次微服务调用的完整链路。**

各微服务向Nacos注册自己的实例，Nacos作为注册中心保存并心跳维持实例状态 -> Gateway使用注册中心的发现结果做基于服务名的路由，由Spring的Reactive LoadBalancer在多实例间负载均衡并转发 -> 服务A使用OpenFeign声明式客户端，底层通过Spring Cloud 的负载均衡与 Nacos 的服务发现，按服务名解析到可用实例并发起 HTTP 调用 -> 若使用 OAuth2/JWT，网关通过透传 Authorization Header 到后端服务；权限判定在后端“资源服务”完成

**Gateway 和服务本身都可以做 JWT 校验，你觉得应该放在哪一层更合适？为什么？**

各服务自行校验JWT，网关可以做粗粒度拦截+透传
- 内部流量也不默认可信；每个资源服务自行鉴权更安全；Spring 官方 Token Relay 的思路也是“网关透传，后端做资源访问控制”
- 把所有鉴权逻辑绑在网关，网关一旦配置失误或被绕过就全线失守；服务端自校验可独立演进。社区与实践也普遍建议资源服务自行做访问控制。
- 业务权限往往与资源、方法、租户、数据域相关，天然适合在资源服务侧做。
- 网关可做快速签名/过期检查拦截明显非法请求，减轻下游压力；但最终的资源级授权仍应在服务内做（两层闸门）。

**你在项目里提到使用 Spring Security + JWT，能否具体说一下登录和鉴权流程？**

登陆：
- 用户提交账号密码到认证端点；由 AuthenticationManager校验。
- 认证成功后生成 JWT，使用私钥/密钥签名，返回给客户端
- 服务端通常不保会话
鉴权：
- 客户端在每次请求携带 Authorization: Bearer <token>
- 在后端服务的 JWT 过滤器中：解析与校验令牌，构造Authentication 放入 SecurityContext，进入 FilterSecurityInterceptor 做基于路径/方法/角色的访问控制。
- 全程无会话，因此每次请求都要带 JWT 并校验一次

🔹 数据库 & 存储

**你在项目中用到 MySQL、PostgreSQL、ES、Redis、MinIO，你会如何设计一个PB 级影像数据管理系统的存储结构？为什么要同时用 ES + MySQL？**

MySQL Minio使用
在卫星项目中，使用MySQL存储影像信息元数据，如影像分辨率、影像存储路径、影像波段数量、影像的空间范围等；使用Minio对象存储实现影像原数据的存储和访问。
我们实现了基于全球可变分辨率网格的影像检索，基于mysql的空间索引，使用网格边界实现影像数据的快速索引。

Redis使用
为适配多用户场景下的影像检索，使用Redis基于HashTable做影像数据的缓存。由于系统中实现了全球动态划分网格，因此在缓存层中基于网格的层级和行列号进行编码作为key，将每景影像的Time+ID作为HashTable中的元素存储。
模型计算（如无云一般图计算）业务，则是通过分布式计算框架Ray实现。用户模块中会为用户分配可使用资源，计算结果会先存入Redis后异步写入mysql

ES使用
使用ES是因为系统内置了一个通过地名地址库搜索影像数据的模块，通过地名的模糊匹配检索poi点位置并索引到相应网格，并获取网格中的影像数据。并考虑到数据增加后使用ES+MySQL实现数据的查询速率优化。

**在 江苏省长江崩岸监测预警系统里提到“分库分表”，你们是如何做分库分表路由的？如何保证跨分片的查询性能？**
这里是基于设备类型分库(基于设备ID)的，然后使用时间维度分表(measure_time字段)，按月分表。

当数据分散在不同的分片中时，查询会遇到路由和聚合的问题，需要尽量避免全局扫描，保证分片路由精确性。
- 首先是在查询时带上分片键而避免全库广播
- 若查询需要跨分片啧需要采用分布式计算框架，通过先计算再汇总的方式
- 可以基于分库分表中间件优化分片查询
- 对于热点或全局查询可以异步写入Redis/ES避免分片聚合
- 针对跨分片代价较高的查询可以定时离线汇总、或使用OLAP系统

**Redis 在你的项目里主要承担什么角色？如果 Redis 挂了，系统如何保证不影响整体服务？**

Redis在我的项目中，主要承担了：
1. 高并发情形下的数据缓存，可减少数据库连接和查询
2. 模型计算临时数据的存储（需要实时修改状态）

若Redis挂了，可以从三个方面讲述不影响整体服务的的做法：
1. 从架构层面
使用哨兵模式，当主节点挂了，哨兵选取一个从节点提升为主节点，客户端只连哨兵获取主库地址，无感切换。也可以通过RedisCluster支持大量数据的分布式存储，每个分片都有主从结构，单点故障自动切换而避免Redis挂掉影响整体服务。
2. 从应用层面看
Redis主要存放的是缓存和临时数据，Redis挂了应用会自动退化为直接访问数据库，并且有熔断+限流保证数据库安全性。
3. 从数据持久层看
若临时数据需要恢复则可以开启AOF追加日志；模型计算服务中，会在内存中存放模型计算的运行时并定期持久化。

🔹 分布式计算 & 中间件

**你在遥感平台中使用了 Ray 框架，能否解释一下 Ray 和传统分布式计算框架（如 Spark）的区别？为什么选择 Ray？**
Spark偏向大数据批处理和流处理，而Ray则是一种通用的分布式执行框架，不仅限于数据处理。
在任务调度上看，Spark是批处理优化的DAG调度器，适合大规模的ETL和SQL分析。而Ray则是基于任务的分布式调度器，适合机器学习计算、在线推理和并行计算。
Spark更偏向于大数据处理，而Ray则专注于AI和科学计算，也支持服务化和弹性计算。

在遥感卫星平台中，需要多景影像在不同网格分辨率下的并行计算，属于大量的独立且并行任务而不是SQL/ETL类型。且地理数据与影像数据需要和Python科学计算库深度结合，Ray对Python支持更加自然。且模型任务都是动态调度且小任务并行的，Spark延迟高不如Ray灵活。

**你们的分布式任务是如何进行调度和容错的？**

Head节点负责任务与服务器资源的分配，Worker节点负责执行提交后的任务。

对于容错机制，Ray的任务是幂等的，worker宕机的任务会被重新分配到其他节点。并且我也在业务侧做了兜底，将临时计算结果写入Redis和Minio，并且任务状态会保存在模型计算队列中，若失败也不会影响全局任务。

**你写过高性能 RPC 框架，请具体说一下服务注册、发现和调用的流程。为什么选择 Etcd，而不是 Nacos/Zookeeper？**
服务注册时，每个服务启动时会将自身的IP，端口，服务名以及版本信息写入Etcd并设置租约，同时开启一个定时任务保证租约不过期。若服务实例宕机则Etcd自动删除节点信息。
服务发现过程：客户端从Etcd中拉取可用节点列表并缓存，同时也会监听etcd变更时间保证服务上线下线的实时感知。
服务调用过程：客户端通过负载均衡策略选择一个节点，基于RPC协议发起远程调用，并支持超时、熔断、重试等，保证高可用。

使用etcd的原因在于：
etcd更轻量，且专注于kv+强一致性
api简单
原生支持watch机制，方便实现服务实时发现。
在k8s生态广泛应用，可靠性和成熟性高。
而nacos性能较etcd若且偏向配置+服务注册一体化，适合微服务体系
zookeeper也同样性能不如etcd

🔹 系统设计 & 性能优化

**你在项目中使用 Docker + Kubernetes 部署，能否说一下 Kubernetes 是如何做服务调度的？如果一个 Pod 崩了，K8s 如何保证服务高可用？**
k8s的服务调度：
使用`schedular`负责pod在node上的调度。
Filter阶段：先过滤掉资源不足或不满足约束的节点
Score阶段：给剩余节点打分，选出最优节点
Bind阶段：将Pod绑定到该节点

若一个Pod发生崩溃，Kubelet会检测到容器异常并重启。
若Node故障，则控制平面则会检测到Node NotReady并在健康的节点上重新调度Pod
结合Deployment/StatefulSet副本机制，确保一个服务有多个副本，单个pod挂掉也不会影响整体服务。
对外通过Service+loadBalancer提供统一访问

**你们的系统涉及 PB 级遥感数据，你是如何设计缓存和索引的，以保证大规模查询性能？**

通过多级缓存来保证大规模的查询性能。
首先，从数据库层面是通过MySQL存储影像元数据，包括基本信息，数据路径，数据空间范围，波段等信息，数据实体则是存放在Minio中。
为应对高并发请求的潜在风险，我们使用了Redis缓存了热点影像数据，通过将全球动态网格进行编码作为key，再使用time和id作为HashTable，存放不同网格中的影像数据。
在后端服务中同时也使用Caffine作为本地缓存，使用了HeavyKeeper算法，使用了多级哈希表的形式，存储较高热点数据。
前端展示过程中，我们使用了云优化Tiff，即在普通的Tiff数据中添加了“Z”型索引，使数据读取过程中不需要获取整幅影像，而是传入一个坐标范围就可用获取部分影像数据，由此实现了基于瓦片层级的影像数据可视化

在数据库查询过程中，使用了MySQL的基于R树的空间索引，数据也按照采集时间简历分区表，保证数据检索过程中命中索引后，获取数据存储位置再定位到对象存储。

**你提到使用 Grafana + Prometheus 做监控，你们会重点监控哪些指标？这些指标触发告警时如何处理？**

重点监控指标有：
- 系统层面：
1. CPU使用率，内存使用率，磁盘I/O，网络带宽
2. 节点存活状态
- k8s层面：
1. Pod状态
2. Pod重启次数
3. Api Server延迟
- 应用层面：
1. 接口请求量，响应时间，错误率
2. Redis命中率，延迟
3. 数据库QPS，慢查询数，连接数
4. Ray分布式任务完成率和失败率

告警分级：
1. 核心服务不可用 —— 工程师立即处理
2. 性能下降 —— 出发自动扩容或降级
3. 资源预警 —— 后续运维优化

🔹 计算机基础

**你的项目涉及大量网络通信，请说一下 TCP 三次握手和四次挥手的过程，以及为什么要有 TIME_WAIT 状态？**

三次握手：
- 客户端 → 服务端：发送 SYN=1，seq=x，请求建立连接。
- 服务端 → 客户端：回 SYN=1, ACK=1，seq=y, ack=x+1，表示同意并同步序列号。
- 客户端 → 服务端：回 ACK=1，ack=y+1，连接建立。
三次原因：必须双向确认，确保双方都具备发送和接收能力，防止历史报文干扰。

四次挥手：
- 客户端 → 服务端：FIN=1，seq=u，请求断开。
- 服务端 → 客户端：ACK=1，ack=u+1，确认请求，但可能还有数据没发完。
- 服务端 → 客户端：FIN=1，seq=v，数据发完了，请求断开。
- 客户端 → 服务端：ACK=1，ack=v+1，确认断开。
四次原因：关闭需要双向独立确认，服务端可能还要发送数据，不能一次完成。

TIME_WAIT状态：
主要出现在主动关闭方（通常是客户端），持续月2msl；
用于确保最后一个ACK能到达，防止旧连接的残留数据包影像新连接。

**在你的 RPC 框架中使用了 Vert.x，请解释一下 事件循环模型，它和 Java 传统线程池有何不同？**

事件循环模型是：
- 基于Reactor模型，核心是少量的EventLoop线程，通常线程数等于CPU核数
- 每个EventLoop线程负责事务的分发，不阻塞
- 开发者编写代码以回调/Future/Promise形式执行，避免线程切换的开销

Java中的传统线程池：
- 每个请求占用一个线程并等待阻塞结构
- 高并发场景下需要大量线程，上下文切换开销大

两者区别在于：
- 线程池每一个请求对应一个线程，而Vert.x是事件驱动+少量线程
- 在I/O密集型场景下，Vertx可以使用少量线程处理大量请求，性能更高
- 计算密集型适合用线程池，I/O密集型则适合用Vertx

**你在简历中提到 Linux 部署经验，请问如何用 top / iostat / netstat 定位一次服务性能瓶颈？**

top可用查看CPU使用率，内存占用，线程数，可以用来判断是CPU的计算瓶颈还是I/O等待高

iostat可以查看磁盘的带宽、等待时间、利用率等

netstat可以用来查看网络连接状态，查看RPC/HTTP请求压力。