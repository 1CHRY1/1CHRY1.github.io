## 虾皮一面

1. 自我介绍

2. 卫星项目拷打：
- 看到你用了蛮多的数据库，问怎么做数据冷热分层的等等
- Ray分布式计算框架怎么做的容错？回答为每个用户限制资源使用。又追问如果不是静态设置而是动态实现高可用如何实现？回答使用监控器监控Ray各节点，任务分配时若服务器性能不满足条件则阻塞
---

Ray有Raylet作为本地节点管理器，GCS作为全局的状态存储，节点会定期与主节点同步资源状态。 Ray框架中主节点有一个全局任务调度器，会接收本地调度器提交的任务并将任务分发给其他节点上合适的本地调度器执行。 使用Redis保存分布式任务的状态信息，包括机器映射，任务描述，任务debug等。 每个Slave上可以启动多个Worker进程执行分布式任务，并将计算结果存储到对象存储中。 每个Slave上启动一个对象存储，Worker通过共享内存的方式访问这些对象数据，可以减少内存的拷贝和对象序列化，底层通过Apache Arrow实现。 每个Slave上的OS都有一个Plasma的对象管理器管理，可以在Worker访问本地OS上不存在的远程数据对象，主动拉取其他Slave上的对象到当前机器。

所以回答的不太对，Ray各节点的Raylet会维护一张资源表，记录可用的CPU，GPU，内存和对象存储容量。因此，Ray框架首先就会把任务提交到global Schedular，若有额外的逻辑判断，可以改写global schedular的源码，在任务选择目标节点前插入自定义评分的逻辑，可以基于业务指标、节点标签或自定义标签选择节点，再由global Schedular将任务派发到节点上执行。

- 看到用了kubernetes，问服务器规模有多大，回答七八个服务器节点；又问网络组件用到是哪个，回答flannel，又问flannel底层是如何实现的？回答不上来了。。

这里详细了解一下k8s网络。

首先，k8s是容器编排平台，设计思路在于聚焦核心编排能力（容器的部署，调度，伸缩，故障自愈等），将网络等非核心功能通过标准化接口交给专业插件实现，CNI即为容器网络接口，k8s只定义网络需求接口。

CNI是一套定义容器网络配置、连接和管理的标准化接口规范。k8s支持多种容器运行时，且可部署在不同的基础设施上，因此k8s内置网络实现需要适配所有运行时和基础设施，而CNI插件可针对特定环境做优化，k8s只需要通过统一接口调用插件，无需关心底层差异。

CNI主要负责完成容器生命周期中与网络相关的操作，核心包括：容器网络创建、网络连接、网络清理、额外扩展等。其工作流程大体为：
kubelet创建pod的容器运行时并准备好容器的网络命名空间用于隔离网络。**（根据namespace创建pod）**
kubelet调用CNI插件（从集群配置的CNI插件路径中读取）并传递Pod的网络命名空间路径、pod信息等参数。**（调用CNI插件并传递pod基本信息）**
CNI插件执行：为Pod分配IP地址，在容器的网络命名空间内创建网络接口eth0并配置IP和路由，将容器的网络接口与宿主机的网络设备关联，确保容器接入集群网络 **（命名空间内子网并与服务器网络设备关联，由此接入集群）**
CNI插件将分配的IP等网络信息返回给kubelet，kubelet完成pod初始化，此时pod就可以正常通信了。**（kubelet获取pod的IP，此时pod就可以与外部通信了）**

CNI与其他组件协同工作：
kube-proxy：依赖CNI提供的POD网络连通性，做service的负载均衡。
CoreDNS：负责k8s内部DNS解析，本身也是pod，依赖CNI接入集群网络。
NetworkPolicy：k8s定义的网络隔离规则，也需要CNI才能生效。

Flannel是k8s网络插件，用于实现pod的跨节点通信，为每个Pod分配一个唯一的子网。Flannel是CNI的一种实现方式。

Flannel是CNI Overlay（虚拟网络覆盖）的网络插件，为每个节点分配pod子网实现全局pod可达。
节点间通过VXLAN隧道或Host-GW路由实现，pod使用虚拟IP透明互通。
使用时，节点使用flanneld守护进程负责管理本地Pod子网和分配IP池，使用etcd存储全局网络配置信息包括各子节点pod子网分配和路由信息，实现了pod创建时网络接口与flannel分配的子网绑定。
同节点的Pod通过直接路由通信，跨节点Pod通过VXLAN封装或物理路由发送
支持动态扩容，通过etcd管理全局网络信息。

Flannel设计极简（不支持网络策略），实现简单只需要一个守护进程，性能中等如VXLAN模式封装解封有性能损耗，兼容性好。Flannel主要是在测试环境/小规模集群中使用，或底层网络首先的环境，不适合生产环境中的网络隔离（POD间通信限制），对网络性能要求较高。

如果有网络性能更高更复杂的需求，可以使用calico或cilium

3. 崩岸项目拷打：
- 问数据如何入网，回答一通最后说使用nginx，又问有没有用过什么配置，回答使用nginx做负载均衡，负载均衡算法等
---

负载均衡算法使用：
upstream: 
轮询，权重轮询，IP哈希，最少连接，加权最少连接，公平，URL哈希等等

静态资源服务：前端Vue或react打包后的静态文件给nginx
缓存：proxy_cache，proxy_cache_valid等字段实现缓存，加速后端接口，减轻数据库或服务压力
安全防护：可以通过limit_req_zone字段限制请求速率，防爬虫，限流防刷，防止暴力请求。
限制连接数：limit_conn字段
SSL/TLS与HTTPS强化：如ssl_{}字段
Gzip压缩/Brotli压缩：减少传输体积，加快页面加载，在高并发环境中有效
跨域处理：add_header..字段
灰度/蓝绿发布：控制一定比例的流量走向新版本
日志与监控：log_format等，可以配合Prometheus做监控与告警。
websocket代理
大文件上传/下载优化：client_max_body_size字段设置大文件上传，断电续传/范围要求（Accept-Ranges bytes）

- 问大文件上传，断点续传如何实现？回答初始版本是用文件系统实现，后面使用minio实现。
minio对文件分片上传是原生自动支持的，只要文件超过阈值（默认100MB），官方客户端/SDK就会自动拆分片，上传并合并，无需用户手动处理。对断点续传功能，服务端能自动存储分片状态，官方工具mc可自动续传。

- 动态数据源切换有没有使用到线程安全的工具，回答使用了ConcurrentHashMap

记住Mybatis使用AbstractRoutingDataSource作为数据源的获取器，每次通过getConnection获取连接。而切换数据源前会使用ThreadLocal将一个DataSource上下文写入线程变量，并在AbstractRoutingDataSource的getConnection方法中设置数据源为ThreadLocal读取到的线程变量。
而所有数据源在查询数据库后都会写入一个ConcurrentHashMap中以便后面调用。

4. RPC项目拷打：
- 问为什么用Vertx不用Netty，回答Vertx是Netty的应用层有Netty的特性。又问Vertx具体有什么特点？回答跨语言兼容，分布式部署等，这个需要深入。

Vertx特点：
事件驱动，通过事件总线传递消息，可以轻松处理大量并发线程。
非阻塞IO，使用Netty的NIO特性。
轻量级与模块化，核心只有事件循环，支持插件化组件，按需加载Verticle（执行单元，相当于微线程）
多语言支持，核心是Java但支持kotlin，js，ruby等语言
高并发友好，线程模型是单线程事件循环+线程池，el处理轻量的非阻塞任务，阻塞任务交给woker线程，避免传统阻塞线程池的上下文切换开销
事件总线，只是进程内、集群内通信，在不同verticle，不同jvm甚至不同机器间通信，非常适合微服务

Netty是底层网络框架，专注于高性能可扩展的网络通信，但只提供了构建网络服务的基础。而Vertx在此基础上提供了开发友好的高层封装。因此大部分项目不直接用 Netty，而是用 Vert.x 来快速构建异步、高并发应用，同时**避免手动管理线程、事件循环和底层通信**细节。

Vertx架构也非常独特
使用Verticle作为垂直单元，相当于一个轻量线程；有Standard Verticle和Worker Verticle作为事件循环/线程池；verticle是部署单元，可以支持热部署和扩缩容
使用EventLoop模型，单线程+非阻塞IO，避免线程切换开销，线程数可配置
使用事件总线，内置集群模式，可以作为轻量级的消息中间件使用
异步API
可扩展和模块化

- 问如果服务挂掉了，会发生什么？回答超时，tcp无法握手等等，但似乎不是他想要的答案
首先该服务接口无法被调用，所有依赖他的下游服务请求都会失败/超时
若没有容错机制，可能下游服务会占用大量线程影响整个系统。
事件/消息丢失，消息积压等问题会出现

可以设置容错机制：
超时设置：每次调用设置合理超时
重试：对短暂失败自动重试，但避免雪崩（偶发失败自动重试）
熔断：使用Hystrix，某个服务连续失败时短路调用（服务连续失败时返回熔断操作结果）
降级：提供备用逻辑或默认值，保证系统基本可用（使用备用结果）
限流/隔离：避免挂掉服务上有服务线程耗尽（对CPU/IO密集服务进行隔离，防止线程耗尽）

- 问代理模式是如何实现的，整体流程，说的有点模糊
创建了一个代理工厂，实现了getProxy方法和getMockProxy方法。在代理类中，根据方法名和服务名和参数类型构建请求，从注册中心获取服务提供者的注册实例，使用负载均衡算法挑选服务后，选用重试机制后发送请求，在catch模块中使用容错机制。

- 问道错误重试机制怎么实现的？问到负载均衡是怎么实现的，回答使用ConcurrentHashMap+spi机制实现

这个项目说的确实不够好，需要多练

1. 看你Java用的比较多，问线程切换后，会记录什么状态？又问线程和进程切换后回记录什么状态

线程是进程内的执行单元，多个线程共享进程的地址空间。

线程切换发生时，操作系统只需要保存/恢复与 **CPU执行**相关的上下文，而无需切换整个内存空间。

线程切换保存的状态有：
硬件上下文：保存/恢复CPU寄存器（程序计数器，线程栈，）
软件上下文：内核栈指针，线程状态，优先级，时间片，TLS基址，信号掩码等
不切换页表，共享地址空间

进程切换保存的状态有：
包含线程切换的内容以外，还有：
内存空间（切换**页表**基址）
进程资源（文件描述符，信号处理器表，命名空间等）

6. 问看你用到Docker和Containerd，你有没有用到cgroup和namespace的特性

namespace和cgroup都是用作资源的隔离。
namespace用来做隔离，让容器看起来像独立的系统。提供进程、网络、文件系统等隔离，让容器看起来像独立机器；
cgroup用来做资源限制和控制，提供 CPU、内存、IO 等资源限制和配额，保证多容器不会互相抢占资源。

本质上来说，docker/containerd底层通过namespace提供隔离，cgroup提供资源限制，再加上镜像层联合文件系统实现容器。

7. concurrenthashmap的底层实现是什么？HashMap是不是线程安全的？回答不是，又追问HashMap扩容的场景，如果一个线程一直给HashMap进行扩容这个可以吗？会发生什么情况？
又问到我Java用的版本，回答17后说，这个特性我这里应该用不到

ConcurrentHashMap
在JDK7使用分段锁将整个表分成若干段，每段一个锁。
在JDK8改为数组+链表/红黑树，使用cas+synchronized（插入删除扩容都是通过cas无锁快速操作，出现竞争时采用synchronized锁定哈希桶的头节点）保证并发安全，扩容时通过多线程协助提升效率。

8. 问了一点cas的底层实现，回答cas用的不多，一般都是直接用sychronize直接锁住，这个没问完

cas是乐观锁的实现方式，首先比较内存中某个值是否等于预期值，若相等则替换为新值，若不等则说明被其他线程改过，更新失败，调用方决定是否重试

底层实现：依赖处理器的原子指令，早期使用锁总线阻止其他CPU访问内存总线，现代CPU基于缓存一致性协议，锁定缓存行而不是整个总线，效率更高。Java通过Unsafe类的compareAndSwap方法调用，JVM在使用时会将方法映射到平台相关的原子指令。

底层逻辑缓存一致性协议：cas操纵要访问x，cpu先将x从内存加载到本地L1/L2缓存；cpu核执行时独占这个缓存行，同故宫MESI协议确保其他核心把这块缓存标记为无效（保证同一时刻只有一个核心可以真正修改这块缓存）；cas在本地缓存中执行，确保在cas完成前没有别的核心修改这个数据；cas更新成功后再把结果写回内存，通知其他核心刷新缓存。

cas内部会加内存屏障，防止读-比较-写顺序不被cpu重排优化后乱序，保证cas更新后结果立刻对其他cpu核心可见

手撕：k个一组翻转链表，c出来了